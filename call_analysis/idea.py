# Stage 1 â€” Extract structured Q/A pairs per call

LLM is best at:

Identifying user questions

Identifying agent responses

Normalizing phrasing

ðŸŽ¯ Output: canonicalized question + canonicalized response


# Stage 2 â€” Aggregate common user questions

We cluster questions across calls using the LLM (semantic dedup).

ðŸŽ¯ Output:

canonical_question

count


# Stage 3 â€” Generate short list of themes

We ask the LLM to produce 5â€“10 themes max, then map questions â†’ themes.

ðŸŽ¯ Output:

theme

questions under theme

counts

# Stage 4 â€” Aggregate agent responses per question

Cluster agent responses per question and count variants.

ðŸŽ¯ Output:

question

response_variant

count
