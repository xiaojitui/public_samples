# Stage 1 â€” Extract structured Q/A pairs per call

LLM is best at:

Identifying user questions

Identifying agent responses

Normalizing phrasing

ðŸŽ¯ Output: canonicalized question + canonicalized response


# Stage 2 â€” Aggregate common user questions

We cluster questions across calls using the LLM (semantic dedup).

ðŸŽ¯ Output:

canonical_question

count


# Stage 3 â€” Generate short list of themes

We ask the LLM to produce 5â€“10 themes max, then map questions â†’ themes.

ðŸŽ¯ Output:

theme

questions under theme

counts

# Stage 4 â€” Aggregate agent responses per question

Cluster agent responses per question and count variants.

ðŸŽ¯ Output:

question

response_variant

count


call_analysis/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ client.py                # already exists (you said)
â”œâ”€â”€ config.py
â”œâ”€â”€ run_pipeline.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ transcripts.csv
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ extract_qa.txt
â”‚   â”œâ”€â”€ cluster_questions.txt
â”‚   â”œâ”€â”€ generate_themes.txt
â”‚   â””â”€â”€ cluster_responses.txt
â”œâ”€â”€ steps/
â”‚   â”œâ”€â”€ step1_extract_qa.py
â”‚   â”œâ”€â”€ step2_cluster_questions.py
â”‚   â”œâ”€â”€ step3_generate_themes.py
â”‚   â””â”€â”€ step4_cluster_responses.py
â””â”€â”€ utils/
    â”œâ”€â”€ io.py
    â””â”€â”€ chunking.py

