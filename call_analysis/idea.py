# Stage 1 â€” Extract structured Q/A pairs per call

LLM is best at:

Identifying user questions

Identifying agent responses

Normalizing phrasing

ðŸŽ¯ Output: canonicalized question + canonicalized response


# Stage 2 â€” Aggregate common user questions

We cluster questions across calls using the LLM (semantic dedup).

ðŸŽ¯ Output:

canonical_question

count


# Stage 3 â€” Generate short list of themes

We ask the LLM to produce 5â€“10 themes max, then map questions â†’ themes.

ðŸŽ¯ Output:

theme

questions under theme

counts

# Stage 4 â€” Aggregate agent responses per question

Cluster agent responses per question and count variants.

ðŸŽ¯ Output:

question

response_variant

count


call_analysis/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ client.py                # already exists (you said)
â”œâ”€â”€ config.py
â”œâ”€â”€ run_pipeline.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ transcripts.csv
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ extract_qa.txt
â”‚   â”œâ”€â”€ cluster_questions.txt
â”‚   â”œâ”€â”€ generate_themes.txt
â”‚   â””â”€â”€ cluster_responses.txt
â”œâ”€â”€ steps/
â”‚   â”œâ”€â”€ step1_extract_qa.py
â”‚   â”œâ”€â”€ step2_cluster_questions.py
â”‚   â”œâ”€â”€ step3_generate_themes.py
â”‚   â””â”€â”€ step4_cluster_responses.py
â””â”€â”€ utils/
    â”œâ”€â”€ io.py
    â””â”€â”€ chunking.py


Theme
 â””â”€â”€ Canonical Question (clustered)
       â””â”€â”€ Agent Response Variants (clustered)


    For each agent:

What themes did they receive calls about?

What questions did they handle under each theme?

How did they respond, and how often?

Where does their response pattern differ from peers?




Layer 1: Semantic schema (slow-changing)
  - Themes
  - Canonical questions
  - Canonical responses

Layer 2: Facts (append-only)
  - (transcript_id, agent_id, canonical_question, canonical_response)

Layer 3: Aggregates (recomputable)
  - final_report
  - agent_report



What should happen when NEW transcripts arrive?
Step 1 â€” Run ONLY the cheap extraction on new data

For new transcripts only:

Extract Q/A pairs

Keep agent_id, transcript_id

Add confidence + reasoning (as before)

No clustering yet.

Step 2 â€” Map new questions to EXISTING canonical questions

For each new question:

Semantic match against existing canonical questions

If similarity â‰¥ threshold â†’ assign

Else â†’ mark as unmapped

This is a retrieval problem, not a clustering problem.

âœ… Fast
âœ… Stable
âœ… Deterministic

Step 3 â€” Map agent responses to canonical responses

Same logic:

Match against existing canonical responses per question

Add new variant only if confidence is low

Step 4 â€” Update aggregates (no LLM needed)

Now update:

final_report counts

agent_report counts

Pure Python.

When do you EVER rerun clustering?

Only when semantic drift exceeds tolerance.

Examples:

10â€“15% of new questions are unmapped

A theme grows too large or incoherent

Business introduces new policy/products

This is a controlled re-index, not a daily job.




We will:

Add LLM-based resolution classification

Add field resolve_question

Compute:
    question_resolved_score =
  sum(resolve_question * response_count) / total_response_count

(Weighted by counts â€” this is important and correct statistically.)


##
low_perf_questions = {
    q["question"]
    for theme in final_report
    for q in theme["questions"]
    if q["question_resolved_score"] < 0.5
}
bad_calls = []

for call in call_reports:
    for q in call["questions"]:
        if q["question"] in low_perf_questions:
            bad_calls.append(call)
            break
##
low_agents = [
    r["agent_id"]
    for r in agent_reports
    if r["overall_resolution_score"] < 0.6
]
##

